{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import os\n",
        "import chardet\n",
        "import joblib\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import xgboost as xgb\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "# import os\n",
        "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "#     for filename in filenames:\n",
        "#         print(os.path.join(dirname, filename))\n",
        "#         path = os.path.join(dirname, filename)\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "id": "8FuQApw5VeNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "dataset_dir = kagglehub.dataset_download(\"abdulmalik1518/mobiles-dataset-2025\")\n",
        "print(\"Path to dataset files:\", dataset_dir)\n",
        "\n",
        "# Find the CSV file within the diectory\n",
        "for filename in os.listdir(dataset_dir):\n",
        "  if filename.endswith(\".csv\"):\n",
        "    csv_path = os.path.join(dataset_dir, filename)\n",
        "    break #Stop after finding the first CSV file\n",
        "print(\"CSV file found at:\", csv_path)\n",
        "\n",
        "with open(csv_path, 'rb') as file:\n",
        "    Chardet = chardet.detect(file.read())\n",
        "print(Chardet['encoding'])"
      ],
      "metadata": {
        "id": "V69d5QJ5v6-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(csv_path, encoding=Chardet['encoding'])\n",
        "df.head()"
      ],
      "metadata": {
        "id": "R83Kj4Gkg-_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail()"
      ],
      "metadata": {
        "id": "x-4T8iKH2tVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Data cleaning and preparations**"
      ],
      "metadata": {
        "id": "Ys7R2B2R2leU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1: Check missing data"
      ],
      "metadata": {
        "id": "ZUrma-jEBXup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "RnMMZaXWhY-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "Pldj0RKHKODd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        " No missing data"
      ],
      "metadata": {
        "id": "HEVz6B4r24V8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2: Check and remove duplications"
      ],
      "metadata": {
        "id": "rE2QgnkqBhCh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "duplication = df.duplicated(keep='first').sum()\n",
        "print(f'Duplicated rows: {duplication}')\n",
        "df = df.drop_duplicates(keep='first')"
      ],
      "metadata": {
        "id": "Q3XcaveO23LM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3: Review each column in dataframe"
      ],
      "metadata": {
        "id": "TV-s2RQIBojh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for col in df.columns:\n",
        "  print(f'Unique values in {col}:\\n', sorted(df[col].dropna().unique()), '\\n')"
      ],
      "metadata": {
        "id": "veWDBdgC3tm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert ['Company Name', 'Model Name', 'Processor'] columns to categorical type\n",
        "categorical_cols = ['Company Name', 'Model Name', 'Processor']\n",
        "df[categorical_cols] = df[categorical_cols].astype('category')\n",
        "\n",
        "# Convert 'Launched Year' to datetime, extract the year, and convert to integer type\n",
        "df['Launched Year'] = pd.to_datetime(df['Launched Year'], format='%Y').dt.year.astype('int')"
      ],
      "metadata": {
        "id": "jQO0jL9sAUCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "kd48wbN4DEDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mobile Weight column: Remove 'g' and convert to float\n",
        "df['Mobile Weight (g)'] = df['Mobile Weight'].str.replace('g', '').astype(float)\n",
        "df.drop('Mobile Weight', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "TNW1afrDEF_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RAM column: Remove 'GB' and convert to float\n",
        "def clean_ram(item):\n",
        "  item = item.replace('GB', '')\n",
        "  if '/' in item:\n",
        "    item = item.split('/')[1] # Take the higher value\n",
        "\n",
        "  return item\n",
        "\n",
        "df['RAM (GB)'] = df['RAM'].apply(clean_ram).astype(float)\n",
        "df.drop('RAM', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "2DyzZ5EFFI8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Front Camera column: Remove 'MP' and convert to float\n",
        "def clean_front_camera(item):\n",
        "  item = [float(i) for i in re.findall(r'\\d+', item)]\n",
        "  return max(item) if item else 0.\n",
        "\n",
        "df['Front Camera (MP)'] = df['Front Camera'].apply(clean_front_camera)\n",
        "df.drop('Front Camera', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "0PYgCdNHGAdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Front Camera column: Remove 'MP', convert to float and split into 4 columns [Main Camera, Ultra Camera, Telephoto Camera, Macro Camera]\n",
        "def clean_back_camera(item):\n",
        "  items = item.split('+')\n",
        "  list_camera = [0., 0., 0., 0.]\n",
        "\n",
        "  for idx, item in enumerate(items):\n",
        "    if idx==2:\n",
        "      if 'macro' not in item.lower():\n",
        "        list_camera[2] = float(item.split('MP')[0])\n",
        "      elif 'macro' in item.lower():\n",
        "        list_camera[3] = float(item.split('MP')[0])\n",
        "    else:\n",
        "      list_camera[idx] = float(item.split('MP')[0])\n",
        "\n",
        "  return list_camera\n",
        "\n",
        "df[['Main Camera (MP)', 'Ultra Camera (MP)', 'Telephoto Camera (MP)', 'Macro Camera (MP)']] = df['Back Camera'].apply(lambda x: pd.Series(clean_back_camera(x)))\n",
        "df.drop('Back Camera', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "OmsCQqdvKEjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Battery Capacity column: Remove 'mAh' and convert to int\n",
        "df['Battery Capacity (mAh)'] = df['Battery Capacity'].str.replace('mAh', '')\n",
        "df['Battery Capacity (mAh)'] = df['Battery Capacity (mAh)'].str.replace(',', '').astype(int)\n",
        "df.drop('Battery Capacity', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "tcynbyAIARK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Screen Size column: Remove 'inches' and convert to float\n",
        "df['Screen Size (inches)'] = df['Screen Size'].apply(lambda x: x.split('inches')[0]).astype(float)\n",
        "df.drop('Screen Size', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "MCNboWZGBXLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Name column: Extract storage size column\n",
        "def extract_storage(item):\n",
        "  item = item.split(' ')[-1]\n",
        "  if 'GB' in item:\n",
        "    item = int(item.replace('GB', ''))\n",
        "  elif 'TB' in item:\n",
        "    item = int(item.replace('TB', '')) * 1024\n",
        "  else:\n",
        "    item = pd.NA\n",
        "\n",
        "  return item\n",
        "\n",
        "df['Storage (GB)'] = df['Model Name'].apply(lambda x: extract_storage(x))"
      ],
      "metadata": {
        "id": "axmb-UJVCVQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in df.columns:\n",
        "  print(f'Unique values in {col}:\\n', sorted(df[col].dropna().unique()), '\\n')"
      ],
      "metadata": {
        "id": "iKMEvltshozn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Launched Price columns: we will create new fields for the country and currency\n",
        "df['Launched Price (China)'] = df['Launched Price (China)'].str.replace('¬•', 'CNY ').str.replace(',', '').str.replace('\\xa0', '')\n",
        "\n",
        "price_columns = [\n",
        "    'Launched Price (Pakistan)', 'Launched Price (India)',\n",
        "    'Launched Price (China)', 'Launched Price (USA)', 'Launched Price (Dubai)'\n",
        "]\n",
        "\n",
        "# Melt the DataFrame: Convert wide format into long format\n",
        "df_melted = df.melt(id_vars=[col for col in df.columns if col not in price_columns],\n",
        "                     value_vars=price_columns,\n",
        "                     var_name=\"Country\", value_name=\"Price\")\n",
        "\n",
        "# Extract Country from column names\n",
        "df_melted[\"Country\"] = df_melted[\"Country\"].str.extract(r'Launched Price \\((.*?)\\)')\n",
        "df_melted['Country'] = df_melted['Country'].astype(\"category\")\n",
        "\n",
        "# Now, df_melted has a single \"Price\" column and a \"Country\" column\n",
        "print(df_melted[['Country', 'Price']].head())\n",
        "\n",
        "# üîπ Replace the original df with the transformed version\n",
        "df = df_melted.copy()"
      ],
      "metadata": {
        "id": "EEAbxm7Zr4VS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract currency and numeric price\n",
        "def split_currency_price(value):\n",
        "    if pd.isna(value) or value.strip() == \"\":\n",
        "        return pd.NA, pd.NA  # Handle missing values\n",
        "\n",
        "    match = re.match(r'([A-Za-z]+)\\s*([\\d,]+)', str(value))  # Extract currency and price\n",
        "    if match:\n",
        "        currency = match.group(1)  # Extract currency (e.g., USD, PKR, INR)\n",
        "        price = match.group(2).replace(',', '')  # Remove commas in numbers\n",
        "        return currency, float(price)  # Convert price to float\n",
        "\n",
        "    return pd.NA, pd.NA  # If no match, return NaN\n",
        "\n",
        "# Apply function to split into two new columns\n",
        "df[['Currency', 'Price (Numeric)']] = df['Price'].apply(lambda x: pd.Series(split_currency_price(x)))\n",
        "\n",
        "# Now, df has separate 'Currency' and 'Price (Numeric)' columns\n",
        "print(df[['Country', 'Price', 'Currency', 'Price (Numeric)']].head())"
      ],
      "metadata": {
        "id": "6zWI2WJG1c8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'Currency' to categorical\n",
        "df['Currency'] = df['Currency'].astype('category')\n",
        "\n",
        "# Convert 'Price (Numeric)' to integer\n",
        "df['Price (Numeric)'] = df['Price (Numeric)'].astype('Int64')  # Supports NaN values\n",
        "\n",
        "# Drop the original 'Price' column\n",
        "df.drop(columns=['Price'], inplace=True)"
      ],
      "metadata": {
        "id": "FlijQ1dZ1g6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display rows where Currency is missing\n",
        "missing_currency_rows = df[df['Currency'].isna()]\n",
        "print(missing_currency_rows['Country'])"
      ],
      "metadata": {
        "id": "lIKySDRT1ioA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[df['Country'] == 'Pakistan', 'Currency'] = 'PKR'\n",
        "df.loc[df['Country'] == 'China', 'Currency'] = 'CNY'"
      ],
      "metadata": {
        "id": "OUBURoqS1tfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display rows where Currency is missing\n",
        "missing_currency_rows = df[df['Currency'].isna()]\n",
        "print(missing_currency_rows['Country'])"
      ],
      "metadata": {
        "id": "68xvjQer1zmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define exchange rates to normalize prices using the currency column\n",
        "exchange_rates = {\n",
        "    'PKR': 0.0036,\n",
        "    'INR': 0.012,\n",
        "    'CNY': 0.14,\n",
        "    'USD': 1.0,\n",
        "    'AED': 0.27\n",
        "}\n",
        "\n",
        "# Function to normalize price\n",
        "def normalize_price(row):\n",
        "    if pd.isna(row['Price (Numeric)']) or pd.isna(row['Currency']):\n",
        "        return np.nan  # Keep NaN if no price or currency\n",
        "    return row['Price (Numeric)'] * exchange_rates.get(row['Currency'], np.nan)\n",
        "\n",
        "# Apply normalization\n",
        "df['Normalized Price (USD)'] = df.apply(normalize_price, axis=1)\n",
        "\n",
        "# Now the prices are normalized\n",
        "print(df[['Price (Numeric)', 'Currency', 'Normalized Price (USD)']].head())\n",
        "# Drop the original 'Price Numeric' column\n",
        "df.drop(columns=['Price (Numeric)'], inplace=True)\n",
        "\n",
        "# Drop the currency column as it isn't needed any more\n",
        "df.drop(columns=['Currency'], inplace=True)"
      ],
      "metadata": {
        "id": "qFg6DWHK15c4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(50)"
      ],
      "metadata": {
        "id": "3TO1K-Nn0UtA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in df.columns:\n",
        "  print(f'Unique values in {col}:\\n', sorted(df[col].dropna().unique()), '\\n')"
      ],
      "metadata": {
        "id": "LWbiTTWn1_sR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets check and see after\n",
        "df.info()"
      ],
      "metadata": {
        "id": "-5OB2PeR2ZxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "NC7br9Ml2JyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The normalized price seems to be missing two records, lets investigate and fillin using the average price grouped by country\n",
        "\n",
        "# Ensure we are working on a copy to avoid SettingWithCopyWarning\n",
        "df = df.copy()\n",
        "\n",
        "# Calculate the average Normalized Price per Country\n",
        "avg_price_per_country = df.groupby('Country', observed=True)['Normalized Price (USD)'].transform('mean')\n",
        "\n",
        "# Fill missing values safely\n",
        "df.loc[df['Normalized Price (USD)'].isna(), 'Normalized Price (USD)'] = avg_price_per_country\n",
        "\n",
        "# Verify if there are still missing values\n",
        "print(df['Normalized Price (USD)'].isna().sum())  # Should print 0 if all missing values are filled"
      ],
      "metadata": {
        "id": "eE_bs1Ku2UZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check summary statistics to look for inconsistancey and to verify\n",
        "print(df[['Mobile Weight (g)', 'Screen Size (inches)', 'Battery Capacity (mAh)', 'Normalized Price (USD)']].describe())"
      ],
      "metadata": {
        "id": "4dhdkh8w4njA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[df['Mobile Weight (g)'] == 732]['Storage (GB)'].unique()"
      ],
      "metadata": {
        "id": "eUXfXMas5GNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For Mobile Weight (g), Max = 732g seems very high (most phones weigh 150-250g)\n",
        "# For Screen Size Max = 14.6 inches and a battery capacity of 11200 mAh is unusually large (could be tablets).\n",
        "df[(df['Screen Size (inches)'] == 14.6) | (df['Mobile Weight (g)'] == 732)][['Storage (GB)', 'Screen Size (inches)', 'Battery Capacity (mAh)', 'Mobile Weight (g)']].drop_duplicates()"
      ],
      "metadata": {
        "id": "LX0tjHBA5MAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets investigate the max price of 39622, it is most likely to be an input error\n",
        "df[df['Normalized Price (USD)'] == 39622][['Storage (GB)','Processor', 'Screen Size (inches)', 'Battery Capacity (mAh)', 'Normalized Price (USD)']].drop_duplicates()"
      ],
      "metadata": {
        "id": "mxtcSOjW5PXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter dataset for similar products (same processor, screen size close to 10.4 inches, and battery capacity close to 8200 mAh)\n",
        "similar_products = df[\n",
        "    (df['Processor'] == 'Unisoc T612') &\n",
        "    (df['Screen Size (inches)'] == 10.4) &  # Screen size around 10.4 inches\n",
        "    (df['Battery Capacity (mAh)'] == 8200)  # Battery capacity around 8200 mAh\n",
        "]\n",
        "\n",
        "# Display the filtered dataset with similar products\n",
        "print(similar_products[['Storage (GB)', 'Processor', 'Screen Size (inches)', 'Battery Capacity (mAh)', 'Normalized Price (USD)']])\n",
        "\n",
        "# Alternatively, you can check the price statistics of the similar products\n",
        "print(similar_products['Normalized Price (USD)'].describe())"
      ],
      "metadata": {
        "id": "Z8MqVT0Q5TkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# It seems like the $39622 is really an input error, lets deal with it\n",
        "\n",
        "# Step 1: Remove the outlier\n",
        "df_filtered = df[(df['Model Name'] != 'T21') | (df['Normalized Price (USD)'] != 39622)]\n",
        "\n",
        "# Step 2: Group by relevant features and calculate the mean price\n",
        "grouped = df_filtered.groupby(['Processor', 'Screen Size (inches)', 'Battery Capacity (mAh)'])['Normalized Price (USD)'].mean().reset_index()\n",
        "\n",
        "# Step 3: Find the correct mean value for the T21 outlier\n",
        "mean_price = grouped[(grouped['Processor'] == 'Unisoc T612') &\n",
        "                     (grouped['Screen Size (inches)'] == 10.4) &\n",
        "                     (grouped['Battery Capacity (mAh)'] == 8200)]['Normalized Price (USD)'].values[0]\n",
        "\n",
        "# Step 4: Update the outlier's price\n",
        "df.loc[(df['Model Name'] == 'T21') & (df['Normalized Price (USD)'] == 39622), 'Normalized Price (USD)'] = mean_price\n",
        "\n",
        "# Step 5: Verify the update\n",
        "updated_price = df[df['Model Name'] == 'T21'][['Model Name', 'Processor', 'Screen Size (inches)', 'Battery Capacity (mAh)', 'Normalized Price (USD)']]\n",
        "print(updated_price)"
      ],
      "metadata": {
        "id": "DArbgOFP5vlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply natural log transformation\n",
        "df['log_normalized_price'] = np.log(df['Normalized Price (USD)'])"
      ],
      "metadata": {
        "id": "XNHPY93V5-XE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['log_normalized_price'].head()"
      ],
      "metadata": {
        "id": "dlws8YIj6GHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4: Vizualization data\n",
        "1: Distribution of variables:"
      ],
      "metadata": {
        "id": "b6jjcWNOXXy7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Histogram: Shows the frequency distribution of a continuous variable. This helps you see the shape of the distribution (e.g., normal, left-skewed, right-skewed, multi-peaked), the central values, and the spread of the data.\n",
        "numerical_cols = ['Mobile Weight (g)',  'Screen Size (inches)',  'Battery Capacity (mAh)',\n",
        "                  'Normalized Price (USD)',  'log_normalized_price',  'RAM (GB)',  'Front Camera (MP)',\n",
        "                  'Main Camera (MP)',  'Ultra Camera (MP)',  'Telephoto Camera (MP)',\n",
        "                  'Macro Camera (MP)',  'Storage (GB)',  'Launched Year']\n",
        "\n",
        "for col in numerical_cols:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.histplot(df[col], kde=True) # kde=True to display the Kernel density estimate line\n",
        "    plt.title(f'Distribution of {col}')\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "StVEIM3I6I7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comment: Observe the distribution shape of 'Normalized Price (USD)' before and after applying log transformation (log_normalized_price). You will see that log transformation makes the distribution more symmetrical."
      ],
      "metadata": {
        "id": "_YPUmnk0fzRT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Box Plot: Displays the quartiles, medians, and potential outliers of  'Normalized Price (USD)',  'log_normalized_price'.\n",
        "\n",
        "for col in [ 'Normalized Price (USD)',  'log_normalized_price']:\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.boxplot(x=df[col])\n",
        "    plt.title(f'Box Plot  {col}')\n",
        "    plt.xlabel(col)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "199FOp6SYnqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2:  Distribution of categorical variables:\n"
      ],
      "metadata": {
        "id": "qfNtjOV7h8ft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bar Chart: Shows the frequency or proportion of each category in a categorical variable.\n",
        "\n",
        "categorical_cols = ['Company Name', 'Country']\n",
        "\n",
        "for col in categorical_cols:\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.countplot(y=df[col], order=df[col].value_counts().index) # order to sort by frequency\n",
        "    plt.title(f'Distribution of  {col}')\n",
        "    plt.xlabel('Frequency')\n",
        "    plt.ylabel(col)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "tE12-KH9gIWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Relationship between variables:"
      ],
      "metadata": {
        "id": "iirsLiXojI98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap: Displays the correlation matrix between continuous variables.\n",
        "\n",
        "for col in numerical_cols:\n",
        "    if df[col].isnull().any():\n",
        "        print(f\"Column '{col}' has missing values. Filling with mean.\")\n",
        "        df[col] = df[col].fillna(df[col].mean())\n",
        "\n",
        "correlation_matrix = df[numerical_cols].corr()\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('The correlation matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VaedVojziawd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both Normalized Price (USD) and log_normalized_price show significant positive correlations with factors such as RAM, storage, year of launch, battery capacity, and screen size. This makes economic and technical sense, as higher-end phones with better features typically cost more."
      ],
      "metadata": {
        "id": "o--YVpDMn1jW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Box Plot by Group: Displays the distribution of Normalized Price (USD) across categories of a categorical variable.\n",
        "\n",
        "plt.figure(figsize=(15, 8))\n",
        "sns.boxplot(x='Company Name', y='Normalized Price (USD)', data=df)\n",
        "plt.title('Price by phone brand')\n",
        "plt.xlabel('Phone brand')\n",
        "plt.ylabel('Price (USD)')\n",
        "plt.xticks(rotation=90)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "86cJWDhLj6bk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Box Plot by Group: Displays the distribution of log_normalized_price across categories of a categorical variable.\n",
        "\n",
        "plt.figure(figsize=(15, 8))\n",
        "sns.boxplot(x='Company Name', y='log_normalized_price', data=df)\n",
        "plt.title('Price by phone brand')\n",
        "plt.xlabel('Phone brand')\n",
        "plt.ylabel('Price')\n",
        "plt.xticks(rotation=90)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zD3fK5RqlUI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Trends over time"
      ],
      "metadata": {
        "id": "P_lTSbh7oCwt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yearly_avg_price = df.groupby('Launched Year')['Normalized Price (USD)'].mean()\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(yearly_avg_price.index, yearly_avg_price.values, marker='o')\n",
        "plt.title('Average price by launched year')\n",
        "plt.xlabel('Launched Year')\n",
        "plt.ylabel('Average price (USD)')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7el70fw1m7j1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For each Company\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "for company in df['Company Name'].unique():\n",
        "    company_data = df[df['Company Name'] == company]\n",
        "    yearly_avg_price_company = company_data.groupby('Launched Year')['Normalized Price (USD)'].mean()\n",
        "    plt.plot(yearly_avg_price_company.index, yearly_avg_price_company.values, marker='o', label=company)\n",
        "\n",
        "plt.title('Average price by launched year and Company Name')\n",
        "plt.xlabel('Launched Year')\n",
        "plt.ylabel('Average price (USD)')\n",
        "plt.legend(title='Company Name', bbox_to_anchor=(1.05, 1), loc='upper left') # Show legend outside the chart\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5MMAG0dXojT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using FacetGrid for compare across countries\n",
        "\n",
        "g = sns.FacetGrid(df, col='Country', col_wrap=3, height=4, aspect=1.5)\n",
        "g.map(sns.lineplot, 'Launched Year', 'Normalized Price (USD)', 'Company Name', marker='o', errorbar=None)\n",
        "g.set_axis_labels('Launched Year', 'Average Price (USD)')\n",
        "g.set_titles(col_template=\"{col_name}\")\n",
        "g.add_legend(title='Company Name', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout(rect=[0, 0, 0.9, 1]) # Adjust layout to make room for legend\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bamlIyyTpYWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data (convert categorical to numeric if not already)\n",
        "df_encoded = df.copy()\n",
        "for col in ['Company Name', 'Model Name', 'Processor', 'Country']:\n",
        "    if df_encoded[col].dtype == 'category':\n",
        "        le = LabelEncoder()\n",
        "        df_encoded[col] = le.fit_transform(df_encoded[col])\n",
        "\n",
        "X = df_encoded.drop(['Normalized Price (USD)', 'log_normalized_price'], axis=1)\n",
        "y = df_encoded['Normalized Price (USD)']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "feature_importances = model.feature_importances_\n",
        "feature_names = X.columns\n",
        "\n",
        "importances_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
        "importances_df = importances_df.sort_values(by='Importance', ascending=False)\n",
        "print(importances_df)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='Importance', y='Feature', data=importances_df)\n",
        "plt.title('Feature Importance from Random Forest')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "m183wWiUqJHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Processor'].value_counts()"
      ],
      "metadata": {
        "id": "Tc_aZb9sZJsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare data for the model:"
      ],
      "metadata": {
        "id": "8WQMBuvEVqp7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Processor'].value_counts()"
      ],
      "metadata": {
        "id": "NNJ6W_fVcVLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1: Based on the value_counts() result of the 'Processor' column, you see that there are many different processors, and most of them appear with very low frequency (only 5 times). This confirms the concern that one-hot encoding this column has created a large number of uninformative features, which can cause dimensionality and overfitting issues.\n",
        "\n",
        "2: The idea: Instead of coding each processor model individually, you can group them by major manufacturer (e.g. Qualcomm Snapdragon, MediaTek Helio/Dimensity, Apple Bionic, Samsung Exynos, Google Tensor, Unisoc, Kirin)."
      ],
      "metadata": {
        "id": "yrVVfgGEcopY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Grouping of processors based on brand:\n",
        "\n",
        "def extract_processor_brand(processor):\n",
        "    if 'Snapdragon' in processor:\n",
        "        return 'Qualcomm Snapdragon'\n",
        "    elif 'MediaTek' in processor or 'Dimensity' in processor or 'Helio' in processor:\n",
        "        return 'MediaTek'\n",
        "    elif 'Bionic' in processor or 'A' in processor:\n",
        "        return 'Apple Bionic'\n",
        "    elif 'Exynos' in processor:\n",
        "        return 'Samsung Exynos'\n",
        "    elif 'Tensor' in processor:\n",
        "        return 'Google Tensor'\n",
        "    elif 'Unisoc' in processor:\n",
        "        return 'Unisoc'\n",
        "    elif 'Kirin' in processor:\n",
        "        return 'Kirin'\n",
        "    else:\n",
        "        return 'Other' # For rare cases\n",
        "\n",
        "df['Processor_Brand'] = df['Processor'].apply(extract_processor_brand)\n",
        "df = pd.get_dummies(df, columns=['Processor_Brand'], drop_first=True)\n",
        "df.drop('Processor', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "VSGMJmgasuia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding categorical variables:\n",
        "\n",
        "df = pd.get_dummies(df, columns=['Company Name', 'Country'], drop_first=False)\n",
        "for col in df.columns:\n",
        "  print(col)"
      ],
      "metadata": {
        "id": "PwDPnpnAV-_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6: Split Data: Split the selected dataset into training set and test set."
      ],
      "metadata": {
        "id": "mS8bjTVihu2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_features = [\n",
        "    'Launched Year', 'Mobile Weight (g)', 'RAM (GB)', 'Front Camera (MP)',\n",
        "    'Main Camera (MP)', 'Ultra Camera (MP)', 'Telephoto Camera (MP)',\n",
        "    'Macro Camera (MP)', 'Battery Capacity (mAh)', 'Screen Size (inches)',\n",
        "    'Storage (GB)'\n",
        "]\n",
        "X = df[numerical_features].copy()\n",
        "y = df['log_normalized_price'].copy()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
        "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
        "\n",
        "# joblib.dump(X_train_scaled.columns, os.path.join(models_dir, 'feature_columns.joblib'))"
      ],
      "metadata": {
        "id": "qJc1sG44b85x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7: Training model"
      ],
      "metadata": {
        "id": "pSNJlCRT83x1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "num_samples = 100\n",
        "\n",
        "pipelines = {\n",
        "    'LinearRegression': Pipeline([\n",
        "        ('regressor', LinearRegression())\n",
        "    ]),\n",
        "    'RandomForestRegressor': Pipeline([\n",
        "        ('regressor', RandomForestRegressor(random_state=42))\n",
        "    ]),\n",
        "    'XGBRegressor': Pipeline([\n",
        "        ('regressor', XGBRegressor(random_state=42, eval_metric='rmse', use_label_encoder=False))\n",
        "    ]),\n",
        "    'LGBMRegressor': Pipeline([\n",
        "        ('regressor', LGBMRegressor(random_state=42))\n",
        "    ])\n",
        "}\n"
      ],
      "metadata": {
        "id": "zdCeEqZP1kuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grids = {\n",
        "    'LinearRegression': {\n",
        "        'regressor__fit_intercept': [True, False]\n",
        "    },\n",
        "    'RandomForestRegressor': {\n",
        "        'regressor__n_estimators': [100, 200],\n",
        "        'regressor__max_depth': [None, 10, 20],\n",
        "        'regressor__min_samples_split': [2, 5]\n",
        "    },\n",
        "    'XGBRegressor': {\n",
        "        'regressor__n_estimators': [100, 200],\n",
        "        'regressor__learning_rate': [0.05, 0.1],\n",
        "        'regressor__max_depth': [3, 5]\n",
        "    },\n",
        "    'LGBMRegressor': {\n",
        "        'regressor__n_estimators': [100, 200],\n",
        "        'regressor__learning_rate': [0.05, 0.1],\n",
        "        'regressor__num_leaves': [31, 62]\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "id": "o76WhqU5A4cp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_models = {}\n",
        "results = []\n",
        "\n",
        "for model_name, pipeline in pipelines.items():\n",
        "    print(f\"\\nƒêang hu·∫•n luy·ªán {model_name}...\")\n",
        "    grid_search = GridSearchCV(\n",
        "        estimator=pipeline,\n",
        "        param_grid=param_grids[model_name],\n",
        "        cv=5, # S·ªë fold cho cross-validation\n",
        "        scoring='neg_mean_squared_error', # S·ª≠ d·ª•ng RMSE (√¢m) ƒë·ªÉ t√¨m ki·∫øm t·ªët nh·∫•t\n",
        "        n_jobs=-1, # S·ª≠ d·ª•ng t·∫•t c·∫£ c√°c core CPU\n",
        "        verbose=1 # Hi·ªÉn th·ªã ti·∫øn tr√¨nh\n",
        "    )\n",
        "\n",
        "    # Hu·∫•n luy·ªán tr√™n d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c scale\n",
        "    grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "    best_estimator = grid_search.best_estimator_\n",
        "    best_params = grid_search.best_params_\n",
        "    best_score = -grid_search.best_score_ # Chuy·ªÉn ƒë·ªïi l·∫°i v·ªÅ RMSE d∆∞∆°ng\n",
        "\n",
        "    print(f\"M√¥ h√¨nh t·ªët nh·∫•t cho {model_name}:\")\n",
        "    print(f\"  Tham s·ªë t·ªët nh·∫•t: {best_params}\")\n",
        "    print(f\"  RMSE tr√™n t·∫≠p hu·∫•n luy·ªán (CV): {np.sqrt(best_score):.4f}\")\n",
        "\n",
        "    # ƒê√°nh gi√° tr√™n t·∫≠p ki·ªÉm tra (ƒë√£ ƒë∆∞·ª£c scale)\n",
        "    y_pred = best_estimator.predict(X_test_scaled)\n",
        "    mse_test = mean_squared_error(y_test, y_pred)\n",
        "    rmse_test = np.sqrt(mse_test)\n",
        "    r2_test = r2_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"  RMSE tr√™n t·∫≠p ki·ªÉm tra: {rmse_test:.4f}\")\n",
        "    print(f\"  R-squared tr√™n t·∫≠p ki·ªÉm tra: {r2_test:.4f}\")\n",
        "\n",
        "    best_models[model_name] = best_estimator\n",
        "    results.append({\n",
        "        'Model': model_name,\n",
        "        'Best Parameters': best_params,\n",
        "        'Train RMSE (CV)': np.sqrt(best_score),\n",
        "        'Test RMSE': rmse_test,\n",
        "        'Test R-squared': r2_test\n",
        "    })\n",
        "\n",
        "# --- 4. T√≥m t·∫Øt k·∫øt qu·∫£ ---\n",
        "\n",
        "print(\"\\n--- T√≥m t·∫Øt k·∫øt qu·∫£ ---\")\n",
        "results_df = pd.DataFrame(results)\n",
        "print(results_df.sort_values(by='Test RMSE'))\n",
        "\n",
        "# B·∫°n c√≥ th·ªÉ truy c·∫≠p c√°c m√¥ h√¨nh t·ªët nh·∫•t nh∆∞ sau:\n",
        "# best_linear_regression = best_models['LinearRegression']\n",
        "# best_random_forest = best_models['RandomForestRegressor']\n",
        "# best_xgb = best_models['XGBRegressor']\n",
        "# best_lgbm = best_models['LGBMRegressor']"
      ],
      "metadata": {
        "id": "NUGObWxyB6cc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Conclusion:\n",
        "\n",
        "- LinearRegression is not suitable for this problem, it only explains less than 50% of the variation in log-normalized prices.\n",
        "\n",
        "- Ensemble tree-based models (RandomForest, XGBoost, LightGBM) give superior performance, explaining more than 90% of the variation. This confirms that the relationship between features and phone prices is non-linear and complex, which tree models can capture better.\n",
        "\n",
        "- LGBMRegressor is the best model among the tested models, achieving the lowest RMSE and highest R-squared on the test set."
      ],
      "metadata": {
        "id": "8Hi6qSwND_wp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ƒê∆∞·ªùng d·∫´n ƒë·∫øn th∆∞ m·ª•c models\n",
        "models_dir = 'models'\n",
        "os.makedirs(models_dir, exist_ok=True) # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a t·ªìn t·∫°i\n",
        "\n",
        "# L∆∞u m√¥ h√¨nh LGBMRegressor t·ªët nh·∫•t\n",
        "best_lgbm_model = best_models['LGBMRegressor']\n",
        "model_path = os.path.join(models_dir, 'best_lgbm_regressor.joblib')\n",
        "joblib.dump(best_lgbm_model, model_path)\n",
        "print(f\"M√¥ h√¨nh LGBMRegressor ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {model_path}\")\n",
        "\n",
        "# L∆∞u StandardScaler\n",
        "scaler_path = os.path.join(models_dir, 'scaler.joblib')\n",
        "joblib.dump(scaler, scaler_path) # 'scaler' l√† bi·∫øn StandardScaler b·∫°n ƒë√£ t·∫°o tr∆∞·ªõc ƒë√≥\n",
        "print(f\"StandardScaler ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {scaler_path}\")\n",
        "\n",
        "joblib.dump(X_train_scaled.columns, os.path.join(models_dir, 'feature_columns.joblib'))"
      ],
      "metadata": {
        "id": "7QBNp8rHCIqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_new_phone_price(new_phone_data_raw, models_dir='models'):\n",
        "    \"\"\"\n",
        "    D·ª± ƒëo√°n gi√° ƒëi·ªán tho·∫°i m·ªõi d·ª±a tr√™n d·ªØ li·ªáu ƒë·∫ßu v√†o th√¥.\n",
        "\n",
        "    Args:\n",
        "        new_phone_data_raw (dict): M·ªôt dictionary ch·ª©a d·ªØ li·ªáu v·ªÅ ƒëi·ªán tho·∫°i m·ªõi.\n",
        "                                   C√°c kh√≥a ph·∫£i kh·ªõp v·ªõi t√™n c·ªôt g·ªëc tr∆∞·ªõc khi ti·ªÅn x·ª≠ l√Ω.\n",
        "                                   V√≠ d·ª•: {'Mobile Weight': '180g', 'RAM': '8GB', ...}\n",
        "        models_dir (str): Th∆∞ m·ª•c ch·ª©a m√¥ h√¨nh v√† scaler ƒë√£ l∆∞u.\n",
        "\n",
        "    Returns:\n",
        "        float: Gi√° ƒëi·ªán tho·∫°i d·ª± ƒëo√°n (tr√™n thang gi√° g·ªëc, kh√¥ng ph·∫£i logarit).\n",
        "        None: N·∫øu c√≥ l·ªói trong qu√° tr√¨nh x·ª≠ l√Ω ho·∫∑c d·ª± ƒëo√°n.\n",
        "    \"\"\"\n",
        "    # Load m√¥ h√¨nh v√† scaler ƒë√£ l∆∞u\n",
        "    try:\n",
        "        best_lgbm_model = joblib.load(os.path.join(models_dir, 'best_lgbm_regressor.joblib'))\n",
        "        scaler = joblib.load(os.path.join(models_dir, 'scaler.joblib'))\n",
        "    except FileNotFoundError:\n",
        "        print(f\"L·ªói: Kh√¥ng t√¨m th·∫•y m√¥ h√¨nh ho·∫∑c scaler trong th∆∞ m·ª•c '{models_dir}'.\")\n",
        "        return None\n",
        "\n",
        "    # T·∫°o DataFrame t·ª´ d·ªØ li·ªáu th√¥\n",
        "    df_new = pd.DataFrame([new_phone_data_raw])\n",
        "\n",
        "    # --- C√°c b∆∞·ªõc ti·ªÅn x·ª≠ l√Ω gi·ªëng h·ªát nh∆∞ trong qu√° tr√¨nh hu·∫•n luy·ªán ---\n",
        "    # B·∫°n ph·∫£i ƒë·∫£m b·∫£o r·∫±ng c√°c b∆∞·ªõc n√†y kh·ªõp ch√≠nh x√°c v·ªõi code ti·ªÅn x·ª≠ l√Ω c·ªßa b·∫°n\n",
        "\n",
        "    # 1. Chuy·ªÉn ƒë·ªïi 'Launched Year'\n",
        "    df_new['Launched Year'] = pd.to_datetime(df_new['Launched Year'], format='%Y').dt.year.astype('int')\n",
        "\n",
        "    # 2. X·ª≠ l√Ω 'Mobile Weight (g)'\n",
        "    df_new['Mobile Weight (g)'] = df_new['Mobile Weight'].str.replace('g', '').astype(float)\n",
        "    df_new.drop('Mobile Weight', axis=1, inplace=True) # X√≥a c·ªôt g·ªëc\n",
        "\n",
        "    # 3. X·ª≠ l√Ω 'RAM (GB)'\n",
        "    def clean_ram(item):\n",
        "        item = item.replace('GB', '')\n",
        "        if '/' in item:\n",
        "            item = item.split('/')[1]\n",
        "        return float(item)\n",
        "    df_new['RAM (GB)'] = df_new['RAM'].apply(clean_ram)\n",
        "    df_new.drop('RAM', axis=1, inplace=True)\n",
        "\n",
        "    # 4. X·ª≠ l√Ω 'Front Camera (MP)'\n",
        "    def clean_front_camera(item):\n",
        "        item = [float(i) for i in re.findall(r'\\d+', str(item))]\n",
        "        return max(item) if item else 0.\n",
        "    df_new['Front Camera (MP)'] = df_new['Front Camera'].apply(clean_front_camera)\n",
        "    df_new.drop('Front Camera', axis=1, inplace=True)\n",
        "\n",
        "    # 5. X·ª≠ l√Ω 'Back Camera' th√†nh 4 c·ªôt\n",
        "    def clean_back_camera(item):\n",
        "        items = item.split('+')\n",
        "        list_camera = [0., 0., 0., 0.]\n",
        "        for idx, sub_item in enumerate(items):\n",
        "            if idx == 2:\n",
        "                if 'macro' not in sub_item.lower():\n",
        "                    list_camera[2] = float(sub_item.split('MP')[0])\n",
        "                elif 'macro' in sub_item.lower():\n",
        "                    list_camera[3] = float(sub_item.split('MP')[0])\n",
        "            else:\n",
        "                list_camera[idx] = float(sub_item.split('MP')[0])\n",
        "        return list_camera\n",
        "\n",
        "    # T·∫°o c√°c c·ªôt t·∫°m th·ªùi tr∆∞·ªõc khi g√°n\n",
        "    temp_cols = ['Main Camera (MP)', 'Ultra Camera (MP)', 'Telephoto Camera (MP)', 'Macro Camera (MP)']\n",
        "    df_new[temp_cols] = df_new['Back Camera'].apply(lambda x: pd.Series(clean_back_camera(x)))\n",
        "    df_new.drop('Back Camera', axis=1, inplace=True)\n",
        "\n",
        "    # 6. X·ª≠ l√Ω 'Battery Capacity (mAh)'\n",
        "    df_new['Battery Capacity (mAh)'] = df_new['Battery Capacity'].str.replace('mAh', '').str.replace(',', '').astype(int)\n",
        "    df_new.drop('Battery Capacity', axis=1, inplace=True)\n",
        "\n",
        "    # 7. X·ª≠ l√Ω 'Screen Size (inches)'\n",
        "    df_new['Screen Size (inches)'] = df_new['Screen Size'].apply(lambda x: x.split('inches')[0]).astype(float)\n",
        "    df_new.drop('Screen Size', axis=1, inplace=True)\n",
        "\n",
        "    # 8. Tr√≠ch xu·∫•t 'Storage (GB)' t·ª´ 'Model Name'\n",
        "    def extract_storage(item):\n",
        "        item = str(item).split(' ')[-1] # ƒê·∫£m b·∫£o l√† string\n",
        "        if 'GB' in item:\n",
        "            return int(item.replace('GB', ''))\n",
        "        elif 'TB' in item:\n",
        "            return int(item.replace('TB', '')) * 1024\n",
        "        else:\n",
        "            return pd.NA\n",
        "\n",
        "    df_new['Storage (GB)'] = df_new['Storage'].apply(extract_storage)\n",
        "\n",
        "    # 9. X·ª≠ l√Ω Processor_Brand (ƒë√¢y l√† m·ªôt b∆∞·ªõc quan tr·ªçng v√¨ b·∫°n ƒë√£ thay th·∫ø Processor g·ªëc)\n",
        "    def extract_processor_brand(processor):\n",
        "        if 'Snapdragon' in processor: return 'Qualcomm Snapdragon'\n",
        "        elif 'MediaTek' in processor or 'Dimensity' in processor or 'Helio' in processor: return 'MediaTek'\n",
        "        elif 'Bionic' in processor or 'A' in processor: return 'Apple Bionic'\n",
        "        elif 'Exynos' in processor: return 'Samsung Exynos'\n",
        "        elif 'Tensor' in processor: return 'Google Tensor'\n",
        "        elif 'Unisoc' in processor: return 'Unisoc'\n",
        "        elif 'Kirin' in processor: return 'Kirin'\n",
        "        else: return 'Other'\n",
        "    df_new['Processor_Brand'] = df_new['Processor'].apply(extract_processor_brand)\n",
        "    df_new.drop('Processor', axis=1, inplace=True) # X√≥a c·ªôt g·ªëc\n",
        "\n",
        "    # 10. One-Hot Encoding cho Processor_Brand, Company Name, Country\n",
        "    # C·∫ßn t·∫°o c√°c c·ªôt OHE gi·ªëng h·ªát nh∆∞ t·∫≠p hu·∫•n luy·ªán.\n",
        "    # C√°ch t·ªët nh·∫•t l√† t·∫°o m·ªôt DataFrame v·ªõi t·∫•t c·∫£ c√°c category c√≥ th·ªÉ c√≥\n",
        "    # v√† sau ƒë√≥ join ho·∫∑c reindex df_new.\n",
        "    # Trong v√≠ d·ª• n√†y, t√¥i s·∫Ω gi·∫£ ƒë·ªãnh b·∫°n ƒë√£ c√≥ danh s√°ch t·∫•t c·∫£ c√°c c·ªôt OHE t·ª´ X_train.\n",
        "    # N·∫øu kh√¥ng, b·∫°n c·∫ßn l∆∞u tr·ªØ danh s√°ch c√°c c·ªôt n√†y trong qu√° tr√¨nh hu·∫•n luy·ªán.\n",
        "\n",
        "    # L·∫•y danh s√°ch ƒë·∫ßy ƒë·ªß c√°c c·ªôt (features) m√† m√¥ h√¨nh ƒë∆∞·ª£c hu·∫•n luy·ªán tr√™n ƒë√≥.\n",
        "    # ƒêi·ªÅu n√†y c·ª±c k·ª≥ quan tr·ªçng ƒë·ªÉ ƒë·∫£m b·∫£o th·ª© t·ª± v√† s·ª± ƒë·∫ßy ƒë·ªß c·ªßa c√°c c·ªôt.\n",
        "    # Trong code hu·∫•n luy·ªán c·ªßa b·∫°n, `X_train_scaled.columns` ch·ª©a t·∫•t c·∫£ c√°c c·ªôt c·∫ßn thi·∫øt.\n",
        "    # B·∫°n c·∫ßn l∆∞u tr·ªØ danh s√°ch n√†y.\n",
        "\n",
        "    # Gi·∫£ ƒë·ªãnh: B·∫°n ƒë√£ l∆∞u danh s√°ch c√°c c·ªôt nh∆∞ sau trong qu√° tr√¨nh hu·∫•n luy·ªán:\n",
        "    # joblib.dump(X_train_scaled.columns, os.path.join(models_dir, 'feature_columns.joblib'))\n",
        "    try:\n",
        "        trained_feature_columns = joblib.load(os.path.join(models_dir, 'feature_columns.joblib'))\n",
        "    except FileNotFoundError:\n",
        "        print(f\"L·ªói: Kh√¥ng t√¨m th·∫•y danh s√°ch c·ªôt ƒë·∫∑c tr∆∞ng ƒë√£ hu·∫•n luy·ªán. Vui l√≤ng l∆∞u `X_train_scaled.columns`.\")\n",
        "        return None\n",
        "\n",
        "    # T√°ch c√°c c·ªôt s·ªë v√† c·ªôt OHE\n",
        "    numerical_features_list = [\n",
        "        'Launched Year', 'Mobile Weight (g)', 'RAM (GB)', 'Front Camera (MP)',\n",
        "        'Main Camera (MP)', 'Ultra Camera (MP)', 'Telephoto Camera (MP)',\n",
        "        'Macro Camera (MP)', 'Battery Capacity (mAh)', 'Screen Size (inches)',\n",
        "        'Storage (GB)'\n",
        "    ]\n",
        "\n",
        "    # T·∫°o c√°c c·ªôt OHE cho df_new\n",
        "    # Danh s√°ch c√°c c·ªôt ph√¢n lo·∫°i (sau khi b·∫°n ƒë√£ t·∫°o Processor_Brand)\n",
        "    categorical_cols_for_ohe = ['Processor_Brand', 'Company Name', 'Country']\n",
        "\n",
        "    # T·∫°o DataFrame t·∫°m th·ªùi ƒë·ªÉ ch·ª©a OHE\n",
        "    df_ohe_temp = pd.get_dummies(df_new[categorical_cols_for_ohe], drop_first=False)\n",
        "\n",
        "    # K·∫øt h·ª£p c√°c c·ªôt s·ªë v√† c√°c c·ªôt OHE\n",
        "    # ƒê·∫£m b·∫£o df_new ch·ªâ ch·ª©a c√°c c·ªôt c·∫ßn thi·∫øt tr∆∞·ªõc khi concat\n",
        "    df_processed = pd.concat([df_new[numerical_features_list], df_ohe_temp], axis=1)\n",
        "\n",
        "    # ƒê·∫£m b·∫£o df_processed c√≥ ch√≠nh x√°c c√°c c·ªôt nh∆∞ `trained_feature_columns`\n",
        "    # ƒê√¢y l√† b∆∞·ªõc c·ª±c k·ª≥ quan tr·ªçng ƒë·ªÉ x·ª≠ l√Ω c√°c category ch∆∞a th·∫•y.\n",
        "    # T·∫°o m·ªôt DataFrame tr·ªëng v·ªõi c√°c c·ªôt mong mu·ªën, sau ƒë√≥ g√°n d·ªØ li·ªáu.\n",
        "    X_processed = pd.DataFrame(columns=trained_feature_columns)\n",
        "\n",
        "    for col in trained_feature_columns:\n",
        "        if col in df_processed.columns:\n",
        "            X_processed[col] = df_processed[col]\n",
        "        else:\n",
        "            X_processed[col] = 0 # ƒêi·ªÅn 0 cho c√°c category ch∆∞a th·∫•y\n",
        "\n",
        "    # X·ª≠ l√Ω NaN (c√≥ th·ªÉ x·∫£y ra n·∫øu Storage (GB) kh√¥ng tr√≠ch xu·∫•t ƒë∆∞·ª£c)\n",
        "    # B·∫°n ƒë√£ d√πng df[col] = df[col].fillna(df[col].mean()) trong ph·∫ßn visualization\n",
        "    # Nh∆∞ng trong prediction, kh√¥ng c√≥ mean c·ªßa to√†n b·ªô t·∫≠p d·ªØ li·ªáu.\n",
        "    # T·ªët nh·∫•t l√† x·ª≠ l√Ω NaN ·ªü ƒë√¢y. V√≠ d·ª•, ƒëi·ªÅn 0 ho·∫∑c m·ªôt gi√° tr·ªã m·∫∑c ƒë·ªãnh h·ª£p l√Ω.\n",
        "    # Ho·∫∑c ƒë·∫£m b·∫£o r·∫±ng d·ªØ li·ªáu ƒë·∫ßu v√†o kh√¥ng c√≥ NaN cho c√°c c·ªôt s·ªë.\n",
        "    # V√≠ d·ª•:\n",
        "    for col in numerical_features_list:\n",
        "        if col in X_processed.columns and X_processed[col].isna().any():\n",
        "            print(f\"C·∫£nh b√°o: C·ªôt '{col}' c√≥ gi√° tr·ªã thi·∫øu. ƒêang ƒëi·ªÅn 0.\")\n",
        "            X_processed[col] = X_processed[col].fillna(0) # Ho·∫∑c gi√° tr·ªã m·∫∑c ƒë·ªãnh kh√°c\n",
        "\n",
        "    # S·∫Øp x·∫øp l·∫°i c·ªôt theo th·ª© t·ª± c·ªßa X_train_scaled.columns\n",
        "    X_processed = X_processed[trained_feature_columns]\n",
        "\n",
        "    # Chu·∫©n h√≥a d·ªØ li·ªáu b·∫±ng scaler ƒë√£ l∆∞u\n",
        "    X_processed_scaled = scaler.transform(X_processed)\n",
        "\n",
        "    # D·ª± ƒëo√°n (tr√™n thang logarit)\n",
        "    log_predicted_price = best_lgbm_model.predict(X_processed_scaled)[0]\n",
        "\n",
        "    # Chuy·ªÉn ƒë·ªïi ng∆∞·ª£c v·ªÅ thang gi√° g·ªëc\n",
        "    predicted_price = np.exp(log_predicted_price)\n",
        "\n",
        "    return predicted_price\n",
        "\n",
        "# --- V√≠ d·ª• c√°ch s·ª≠ d·ª•ng h√†m d·ª± ƒëo√°n ---\n",
        "# D·ªØ li·ªáu m·∫´u (thay th·∫ø b·∫±ng d·ªØ li·ªáu ƒëi·ªán tho·∫°i th·ª±c t·∫ø c·ªßa b·∫°n)\n",
        "# R·∫§T QUAN TR·ªåNG: C√°c kh√≥a trong dictionary n√†y ph·∫£i l√† T√äN C·ªòT G·ªêC c·ªßa b·∫°n\n",
        "# tr∆∞·ªõc khi c√°c b∆∞·ªõc l√†m s·∫°ch v√† ti·ªÅn x·ª≠ l√Ω b·∫Øt ƒë·∫ßu.\n",
        "new_phone_data = {\n",
        "    'Launched Year': 2024,\n",
        "    'Mobile Weight': '190g',\n",
        "    'RAM': '12GB',\n",
        "    'Front Camera': '32MP',\n",
        "    'Back Camera': '108MP+12MP+5MP', # V√≠ d·ª•: Main+Ultra+Macro (kh√¥ng Telephoto)\n",
        "    'Battery Capacity': '5000mAh',\n",
        "    'Screen Size': '6.7 inches',\n",
        "    'Storage': '256GB',\n",
        "    'Processor': 'Qualcomm Snapdragon 8 Gen 3',\n",
        "    'Company Name': 'Samsung',\n",
        "    'Country': 'USA' # Ph·∫£i l√† m·ªôt trong c√°c qu·ªëc gia b·∫°n ƒë√£ hu·∫•n luy·ªán\n",
        "}\n",
        "\n",
        "# Tr∆∞·ªõc khi ch·∫°y h√†m n√†y, h√£y ƒë·∫£m b·∫£o b·∫°n ƒë√£ l∆∞u `X_train_scaled.columns`\n",
        "# trong qu√° tr√¨nh hu·∫•n luy·ªán c·ªßa b·∫°n ƒë·ªÉ h√†m c√≥ th·ªÉ t·∫£i n√≥ l√™n.\n",
        "# V√≠ d·ª•, sau d√≤ng `X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)`\n",
        "# B·∫°n th√™m d√≤ng sau:\n",
        "# joblib.dump(X_train_scaled.columns, os.path.join(models_dir, 'feature_columns.joblib'))\n",
        "\n",
        "\n",
        "# Sau khi b·∫°n ƒë√£ ch·∫°y code tr√™n ƒë·ªÉ l∆∞u m√¥ h√¨nh v√† scaler:\n",
        "predicted_price = predict_new_phone_price(new_phone_data)\n",
        "\n",
        "if predicted_price is not None:\n",
        "    print(f\"\\nGi√° d·ª± ƒëo√°n c·ªßa ƒëi·ªán tho·∫°i m·ªõi l√†: ${predicted_price:.2f}\")"
      ],
      "metadata": {
        "id": "UjJ841CyIJrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UYnRMzNcIwQW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}